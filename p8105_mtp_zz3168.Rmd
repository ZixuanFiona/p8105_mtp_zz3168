---
title: "p8105_mtp_zz3168"
author: "Zixuan Zhang"
date: "2023-10-20"
output: github_document
---

```{r}
library(readr)
library(readxl)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidyr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 10, 
  fig.height = 9,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 
##Data import, cleaning, and quality control
We import, organize, and merge data sets, and then delete erroneous data from the merged table through identification. Then we use statistics and visualization to analyze and identify trends in the data set.

The original data includes New York City's COA data from 2018 to 2022 and `zip_data` data that accurately reflects New York City's counties, boroughs, and communities. These include `borough` variables, `net_change` variable, `zip_code`, `month` and other important data.

We first synthesize the COA data from 2018 to 2022 in `coa_data`, then convert the time into `month`, `year`, `date`, and use `total_perm_in - total_perm_out` to get the data of `net_change`. By combining the data of `zip_data` and `coa_data`, we found that some erroneous overlaps appeared in the total data table, and obtained a new data table through exclusion.


```{r}
coa_data_1 = read_xlsx("data/USPS CHANGE OF ADDRESS NYC.xlsx", sheet = c("2018")) 
  coa_data_2 = read_xlsx("data/USPS CHANGE OF ADDRESS NYC.xlsx", sheet = c("2019")) 
coa_data_3 = read_xlsx("data/USPS CHANGE OF ADDRESS NYC.xlsx", sheet = c("2020")) 
coa_data_4 = read_xlsx("data/USPS CHANGE OF ADDRESS NYC.xlsx", sheet = c("2021")) 
coa_data_5 = read_xlsx("data/USPS CHANGE OF ADDRESS NYC.xlsx", sheet = c("2022"))

coa_data = 
  bind_rows(coa_data_1,coa_data_2,coa_data_3,coa_data_4,coa_data_5) |> 
  janitor::clean_names() |> 
  mutate(time = month) |> 
  separate(month, into = c("year", "month", "date") ) |> 
  mutate(net_change = total_perm_in - total_perm_out) |> 
  rename(zip_code = zipcode) 
 



zip_data = read_csv("data/Zip Codes.csv") |> 
  janitor::clean_names() |> 
  mutate(borough = ifelse(county_name == "New York", "Manhattan", county_name)) 


  
combine_data = 
  inner_join(zip_data, coa_data,by="zip_code") |> 
  filter(!(borough == "Manhattan" & zip_code == 10463) &
          !(borough == "Manhattan" & zip_code == 11201) &
          !(borough == "Queens" & zip_code == 11239) &
          !(borough == "Kings" & zip_code == 11693))
  
  

```

##describe Tidy Dataset
Through the combined data table `combine_data` we get that there are a total of `r nrow(combine_data)` data, including 237 unique postal codes and 42 unique communities

```{r}
#unique_zips
combine_data |> 
  select(zip_code) |> 
  distinct() |> 
  head(n = 10) |> 
  knitr::kable()

#unique_neighborhoods
combine_data |> 
  drop_na() |> 
  select(neighborhood) |> 
  distinct() |> 
  knitr::kable()
  

```

##Compare city
By comparing Manhattan and Queens, we found that New York has the largest proportion in Manhattan, with 3477, followed by CANAL STREET, ROOSEVELT ISL, etc. Their data are very different. In the data of Queens, JAMAICA accounts for the largest proportion at 372, followed by FLUSHING, ASTORIA, etc., but their data are not very different.

```{r}
#manhateen
combine_data |> 
  filter(borough == "Manhattan") |> 
  count(city) |> 
  arrange(desc(n)) |> 
  mutate(rank = row_number()) |> 
  filter(rank < 7) |> 
  knitr::kable()
  
```

```{r}
#queen
combine_data |> 
  filter(borough == "Queens") |> 
  count(city) |> 
  arrange(desc(n)) |> 
  mutate(rank = row_number()) |> 
  filter(rank < 7) |> 
  knitr::kable()
```


## Problem 2
#EDA
We created a table to show the averages for each borough and year. According to the `average_df` data, Manhattan's average has the largest difference, and Richmond's average has the smallest difference.
```{r}
combine_data |> 
  group_by(borough, year) |> 
  summarize(average_df = mean(net_change, na.rm = TRUE)) |> 
  mutate(year = as.integer(year)) |> 
  knitr::kable()


```
##lowest values and highest 
Two tables were created to show the five lowest values in the data and the five highest values for 2020. The lowest value in `net_change` is -983, and the highest value in 2020 is 360 in `net_change`.
```{r}
#lowest
combine_data |> 
  arrange(net_change) |> 
  select(zip_code, neighborhood, year, month, net_change) |> 
  head(n = 5) |> 
  knitr::kable()

#highest
combine_data |> 
  filter(year < 2020) |> 
  arrange(desc(net_change)) |> 
  select(zip_code, neighborhood, year, month, net_change) |> 
  head(n = 5) |> 
  knitr::kable()

  

```


## Visualization:
Based on the ggplot chart, we can derive the community-level average versus month over the five years. Manhattan still has the most data among the five communities, but its average difference in 2020 is larger than the other four communities. Richmond's average has the least correlation with the month, but its average is much more stable than the other four communities.
```{r}
combine_plot = 
  combine_data |> 
  mutate(month = as.integer(month),
         year = as.integer(year))


combine_plot |> 
  group_by(year, time, neighborhood, borough) |> 
  summarize(average_change = mean(net_change, na.rm = TRUE)) |> 
  ggplot(aes(x = time, 
             y = average_change, 
             color = neighborhood)
       ) +
  geom_point()+
  geom_line() + 
  facet_wrap(~borough) +
  labs(title = "Monthly Net Change across Neighborhoods", 
       x = "Time", 
       y = "Average Net Change")



# Export the plot
ggsave("results/city change.pdf")

```

